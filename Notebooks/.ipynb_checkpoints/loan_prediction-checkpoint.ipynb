{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e81754-86ed-4e7f-9ade-fd72480f3987",
   "metadata": {},
   "source": [
    "## Мотивација и методолошки пристап\r\n",
    "\r\n",
    "Целта на овој проект е да се изгради модел кој ќе предвидува дали заемопримачот ќе го врати кредитот (`loan_paid_back`). Овој проблем не е само технички, туку и бизнис-критичен, бидејќи различните типови на грешки носат различен ризик. Во реален кредитен систем, многу е поопасно да се одобри кредит на лице кое нема да го врати, отколку да се одбие кредит на лице кое реално би го вратило. Поради тоа, во овој проект се применува cost-sensitive пристап, каде што лажно позитивните предвидувања (false positives) се сметаат за пет пати поскапи од лажно негативните (false negatives).\r\n",
    "\r\n",
    "Податоците содржат нумеричккатегорискиални карактеристики, а различните модели имаат различни барања во однос на начинот на нивна обработка. Моделите како Logistic Regression и Multilayer Perceptron се чувствителни на размерите на вредностите и бараат стандардизација на нумеричките податоци за да функционираат стабилно и ефикасно. Од друга страна, tree-based моделите како Random Forest, XGBoost и LightGBM не зависат од скалата на карактеристиките на ист начин, бидејќи одлуките ги носат преку поделби (splits) во дрва. Поради тоа се користат две различни preprocessing pipeline-и: еден со стандардизација за модели чувствителни на скала и еден без стандардизација за tree-based модели, при што во двата случаи категоријалните променливи се претвораат во нумерички преку One-Hot Encoding.\r\n",
    "\r\n",
    "Наместо да се користи стандарден праг од 0.5 за носење одлука, секој модел се евалуира преку оптимизација на прагот врз основа на дефинираната cost функција. Ова значи дека одлуката дали еден заем ќе биде одобрен или не не се базира само на веројатноста што ја дава моделот, туку и на реалниот ризик поврзан со можната грешка. Како резултат на ваквиот пристап, очекувано е моделите да бидат поконзервативни и да користат високи прагови за позитивна класа, со цел да се минимизира бројот на ризични одобрувања.\r\n",
    "\r\n",
    "Покрај поединечните модели, изграден е и ансамбл модел преку soft voting, каде што се комбинираат веројатностите од сите модели. Мотивацијата за ансамблот е дека различните модели можат да научат различни шеми и релации во податоците, а нивната комбинација може да доведе до постабилни и поробустни предвидувања. Сепак, ансамблот не се третира како автоматски подобро решение, туку како дополнителен чекор кој се евалуира под истите cost-sensitive критериуми како и останатите модели.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b29b6c1-6fa2-470c-b0be-776eb31cb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6b5cb5-6700-4694-aeaf-d79b598f1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"loan_dataset_20000.csv\") \n",
    "\n",
    "X_raw = df.drop(\"loan_paid_back\", axis=1)\n",
    "y = df[\"loan_paid_back\"]\n",
    "\n",
    "# monthly and yearly income carry the same info monhly = yearly / 12\n",
    "X_raw = X_raw.drop(columns=['monthly_income'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6d6dd7-4dc0-44b9-9f38-4034ead73dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_raw.copy()\n",
    "y = y.copy()\n",
    "\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f18047-38e8-4509-adf8-b5e49d66c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "preprocessor_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7114031-1a27-45fd-b7da-9ef94ef6141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (16000, 21) (16000,)\n",
      "Test shapes:  (4000, 21) (4000,)\n",
      "\n",
      "Class distribution (train):\n",
      " loan_paid_back\n",
      "1    0.799875\n",
      "0    0.200125\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution (test):\n",
      " loan_paid_back\n",
      "1    0.8\n",
      "0    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "target_col = \"loan_paid_back\" \n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# stratify keeps the 0/1 ratio similar in train and test (important for FP/FN work)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes: \", X_test.shape, y_test.shape)\n",
    "print(\"\\nClass distribution (train):\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution (test):\\n\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8b025-5a3e-4112-bf7d-3f6e22af6ee9",
   "metadata": {},
   "source": [
    "### Helper functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bdfc4b-a0c4-48db-890a-f3a26251b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "COST_FP = 5\n",
    "COST_FN = 1\n",
    "\n",
    "def cost_and_cm(y_true, y_pred, cost_fp=5, cost_fn=1):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    cost = cost_fp * fp + cost_fn * fn\n",
    "    return cost, (tn, fp, fn, tp)\n",
    "\n",
    "def best_threshold_by_cost(y_true, proba_pos, cost_fp=5, cost_fn=1):\n",
    "    thresholds = np.linspace(0.01, 0.99, 199)\n",
    "    best = {\"threshold\": 0.5, \"cost\": float(\"inf\"), \"cm\": None}\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (proba_pos >= t).astype(int)\n",
    "        cost, cm = cost_and_cm(y_true, y_pred, cost_fp, cost_fn)\n",
    "        if cost < best[\"cost\"]:\n",
    "            best = {\"threshold\": float(t), \"cost\": float(cost), \"cm\": cm}\n",
    "    return best\n",
    "\n",
    "def print_best(name, best):\n",
    "    tn, fp, fn, tp = best[\"cm\"]\n",
    "    print(f\"{name} | best threshold = {best['threshold']:.3f} | cost = {best['cost']:.0f}\")\n",
    "    print(f\"{name} | confusion (tn, fp, fn, tp) = {tn}, {fp}, {fn}, {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68d874-9d44-449f-a3f6-ce4a3783933a",
   "metadata": {},
   "source": [
    "### Logistic regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab93d92-8706-4741-849e-13a6854198b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg | best threshold = 0.990 | cost = 1626\n",
      "LogReg | confusion (tn, fp, fn, tp) = 564, 236, 446, 2754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "lr_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_scaled),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        class_weight={0: 1, 1: COST_FP}  # pushes toward fewer FP at a given threshold\n",
    "    ))\n",
    "])\n",
    "\n",
    "# optional sample weights (extra cost-sensitivity)\n",
    "sw = compute_sample_weight(class_weight={0: 1, 1: COST_FP}, y=y_train)\n",
    "\n",
    "# LR supports sample_weight\n",
    "lr_pipe.fit(X_train, y_train, model__sample_weight=sw)\n",
    "\n",
    "p_lr = lr_pipe.predict_proba(X_test)[:, 1]\n",
    "best_lr = best_threshold_by_cost(y_test, p_lr, COST_FP, COST_FN)\n",
    "print_best(\"LogReg\", best_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206bce9-6d91-4aba-8e3b-d4837cd80c91",
   "metadata": {},
   "source": [
    "### MLP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8efc0f11-3588-4df5-ae0a-fba53fc1d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP used sample_weight: True\n",
      "MLP | best threshold = 0.970 | cost = 1613\n",
      "MLP | confusion (tn, fp, fn, tp) = 654, 146, 883, 2317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mlp_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_scaled),\n",
    "    (\"model\", MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        early_stopping=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "sw = compute_sample_weight(class_weight={0: 1, 1: COST_FP}, y=y_train)\n",
    "\n",
    "# MLPClassifier supports sample_weight in sklearn versions that are reasonably recent.\n",
    "# If yours errors, remove model__sample_weight and rely on thresholding.\n",
    "try:\n",
    "    mlp_pipe.fit(X_train, y_train, model__sample_weight=sw)\n",
    "    used_sw = True\n",
    "except TypeError:\n",
    "    mlp_pipe.fit(X_train, y_train)\n",
    "    used_sw = False\n",
    "\n",
    "p_mlp = mlp_pipe.predict_proba(X_test)[:, 1]\n",
    "best_mlp = best_threshold_by_cost(y_test, p_mlp, COST_FP, COST_FN)\n",
    "print(\"MLP used sample_weight:\", used_sw)\n",
    "print_best(\"MLP\", best_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0d97a-ac50-4b49-b0d5-529d99c38cc8",
   "metadata": {},
   "source": [
    "### Random forrest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fe093e-58b2-4ad8-a195-0311eee30ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest | best threshold = 0.752 | cost = 1674\n",
      "RandomForest | confusion (tn, fp, fn, tp) = 542, 258, 384, 2816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_tree),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=600,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight={0: 1, 1: COST_FP}\n",
    "    ))\n",
    "])\n",
    "\n",
    "sw = compute_sample_weight(class_weight={0: 1, 1: COST_FP}, y=y_train)\n",
    "\n",
    "# RF supports sample_weight\n",
    "rf_pipe.fit(X_train, y_train, model__sample_weight=sw)\n",
    "\n",
    "p_rf = rf_pipe.predict_proba(X_test)[:, 1]\n",
    "best_rf = best_threshold_by_cost(y_test, p_rf, COST_FP, COST_FN)\n",
    "print_best(\"RandomForest\", best_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721439d5-84c4-4aa9-b15f-90fd4f329705",
   "metadata": {},
   "source": [
    "### XGBoost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b3836f-be9e-4ee9-8d7a-0ac15717320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost | best threshold = 0.945 | cost = 1587\n",
      "XGBoost | confusion (tn, fp, fn, tp) = 576, 224, 467, 2733\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_tree),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "sw = compute_sample_weight(class_weight={0: 1, 1: COST_FP}, y=y_train)\n",
    "\n",
    "# XGB supports sample_weight\n",
    "xgb_pipe.fit(X_train, y_train, model__sample_weight=sw)\n",
    "\n",
    "p_xgb = xgb_pipe.predict_proba(X_test)[:, 1]\n",
    "best_xgb = best_threshold_by_cost(y_test, p_xgb, COST_FP, COST_FN)\n",
    "print_best(\"XGBoost\", best_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c394ae-7c8f-4d14-af2f-c2c42ad02253",
   "metadata": {},
   "source": [
    "### LightGBM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98329f89-37ad-461a-aef3-2ca21dbba8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12798, number of negative: 3202\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2251\n",
      "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.990091 -> initscore=4.604389\n",
      "[LightGBM] [Info] Start training from score 4.604389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakim\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM | best threshold = 0.990 | cost = 1644\n",
      "LightGBM | confusion (tn, fp, fn, tp) = 609, 191, 689, 2511\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "lgbm_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_tree),\n",
    "    (\"model\", LGBMClassifier(\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=31,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        class_weight={0: 1, 1: COST_FP}\n",
    "    ))\n",
    "])\n",
    "\n",
    "sw = compute_sample_weight(class_weight={0: 1, 1: COST_FP}, y=y_train)\n",
    "\n",
    "# LGBM supports sample_weight\n",
    "lgbm_pipe.fit(X_train, y_train, model__sample_weight=sw)\n",
    "\n",
    "p_lgbm = lgbm_pipe.predict_proba(X_test)[:, 1]\n",
    "best_lgbm = best_threshold_by_cost(y_test, p_lgbm, COST_FP, COST_FN)\n",
    "print_best(\"LightGBM\", best_lgbm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e0c3b-80a4-43ca-ac7a-11414068325c",
   "metadata": {},
   "source": [
    "### Soft voting ensemble ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148b1352-9648-4b92-83d6-d9a470d9e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble(soft) | best threshold = 0.945 | cost = 1616\n",
      "Ensemble(soft) | confusion (tn, fp, fn, tp) = 631, 169, 771, 2429\n"
     ]
    }
   ],
   "source": [
    "# Equal-weight soft vote\n",
    "p_ens = (p_lr + p_mlp + p_rf + p_xgb + p_lgbm) / 5.0\n",
    "\n",
    "best_ens = best_threshold_by_cost(y_test, p_ens, COST_FP, COST_FN)\n",
    "print_best(\"Ensemble(soft)\", best_ens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7de16a-0d15-49d9-9029-bfec8c07bc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
